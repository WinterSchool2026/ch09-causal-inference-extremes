{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Machine Learning (DML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DML is an algorithm that applies machine learning methods to fit the treatment and response, then uses a linear model to predict the response residuals from the treatment residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from econml.dml import LinearDML, CausalForestDML\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, roc_auc_score\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample data (trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../results/causal/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "samples = pd.read_csv(\"../data/df_ps_trimmed.csv\")\n",
    "\n",
    "print(f\"Shape of sampled data: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the variables: Outcome (target), Treatment, Heterogeneity (zones), Confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['DI_agri_extreme_M7']\n",
    "\n",
    "treatment = ['SMA_2']\n",
    "\n",
    "zones = ['basin_lv2']\n",
    "\n",
    "vars_list = ['E_gleam_ds','S_gleam_ds','H_gleam_ds',\n",
    "            'pev_ds','sro_ds','sp_ds','tp_ds','d2m_ds',\n",
    "            'agri_irri', 'agri_mix', 'agri_rain',\n",
    "            'soil_clay', 'soil_oc', 'soil_roots','soil_sand', 'soil_tawc',\n",
    "            'lst_night_ds','ndvi_ds','ndwi_ds',\n",
    "            'pop','road','hand','lc2','lc3','lc5','lc8',\n",
    "            'censo','soi_long','pdo_timeseries_sstens','noaa_globaltmp_comb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Xi (the heterogeneity features) - Create a one-hot ecoder (dummy variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_encoded = encode_categorical_raster(samples[zones[0]], prefix='zone')\n",
    "samples_zones = samples.join(zones_encoded)\n",
    "zone_vars = [v for v in samples_zones.columns if v.startswith(('zone'))]\n",
    "samples_zones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show proportion of true/negatives for treatment/outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix using crosstab\n",
    "# Rows = Treatment, Columns = Target\n",
    "ct = pd.crosstab(samples_zones[treatment[0]], samples_zones[target[0]])\n",
    "    \n",
    "# Extract values using .loc[row, col]\n",
    "NN = ct.loc[0, 0] # Treatment 0, Outcome 0\n",
    "NP = ct.loc[1, 0] # Treatment 1, Outcome 0 (T=1, O=0)\n",
    "PN = ct.loc[0, 1] # Treatment 0, Outcome 1 (T=0, O=1)\n",
    "PP = ct.loc[1, 1] # Treatment 1, Outcome 1 (T=1, O=1)\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append({\n",
    "    'Treatment': treatment,\n",
    "    '(T=0, O=0)': NN,\n",
    "    '(T=0, O=1)': PN,\n",
    "    '(T=1, O=0)': NP,\n",
    "    '(T=1, O=1)': PP,\n",
    "    'Tot_P_Treat': NP + PP,\n",
    "    'Tot_P_Out': PN + PP,\n",
    "})\n",
    "\n",
    "# 3. Display the summary dataframe\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(\"Outcome:\")\n",
    "print(samples_zones[target[0]].value_counts())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Treatment and Outcome for each Heterogenous zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize a list to store results for all zones\n",
    "target_col = target[0] if isinstance(target, list) else target\n",
    "all_zone_results = []\n",
    "\n",
    "# 2. Loop through each zone and calculate the confusion matrix\n",
    "for zone in zone_vars:\n",
    "    # Generate crosstab: Row = Zone Presence (0 or 1), Column = Outcome (0 or 1)\n",
    "    ct = pd.crosstab(samples_zones[zone], samples_zones[target_col])\n",
    "    \n",
    "    # Ensure all quadrants exist (2x2)\n",
    "    ct = ct.reindex(index=[0, 1], columns=[0, 1], fill_value=0)\n",
    "    \n",
    "    # Append the results for this specific zone\n",
    "    all_zone_results.append({\n",
    "        'Zone': zone,\n",
    "        '(T=0, O=0)': ct.loc[0, 0],  # Outside this zone, no outcome\n",
    "        '(T=0, O=1)': ct.loc[0, 1],  # Outside this zone, outcome exists\n",
    "        '(T=1, O=0)': ct.loc[1, 0],  # Inside this zone, no outcome\n",
    "        '(T=1, O=1)': ct.loc[1, 1]   # Inside this zone, outcome exists\n",
    "    })\n",
    "\n",
    "# 3. Create the consolidated summary DataFrame\n",
    "summary_df = pd.DataFrame(all_zone_results).set_index('Zone')\n",
    "\n",
    "# 4. Define colors\n",
    "# Blue/Light Blue for Negatives, Orange/Gold for Positives\n",
    "colors = ['#aec7e8', '#1f77b4', '#ebb95e', '#d18b00']\n",
    "\n",
    "# 5. Plot everything in one figure\n",
    "fig, ax = plt.subplots(figsize=(9, 5), dpi=100)\n",
    "\n",
    "summary_df[['(T=0, O=0)', '(T=0, O=1)', '(T=1, O=0)', '(T=1, O=1)']].plot(\n",
    "    kind='bar', \n",
    "    stacked=True, \n",
    "    color=colors, \n",
    "    ax=ax, \n",
    "    edgecolor='white',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# Formatting\n",
    "plt.title('Data Imbalance across Regions (Zones)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Observations', fontsize=12)\n",
    "plt.xlabel('Zone (heterogeneity)', fontsize=12)\n",
    "plt.legend(title='(Treatment,Outcome)', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize spatial bias (T/O) to check if we have enough representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spatial_bias(samples_zones, treatment[0], target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Machine Learning (EconML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Machine Learning (DML) is an algorithm that applies machine learning methods to fit the treatment and response, then uses a model to predict the response residuals from the treatment residuals.\n",
    "\n",
    "The EconML SDK implements the following DML classes:\n",
    "\n",
    "- **LinearDML**: suitable for estimating heterogeneous treatment effects.\n",
    "\n",
    "- **CausalForestDML**: ML algorithm used to estimate heterogeneous treatment effects (Conditional Average Treatment Effect, CATE). By combining Double Machine Learning (DML) with Causal Forests. It is a popular estimator for causal inference, designed to identify how a treatment effect varies across different segments of a population based on individual characteristics, even in high-dimensional settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y$ – Outcomes for each sample\n",
    "\n",
    "$T$ – Treatments for each sample\n",
    "\n",
    "$X$ – Features for each sample (Regions, heterogeneity). To find variation (CATE). These are the variables where you suspect the treatment effect might be different. Encoded one-hot\n",
    "\n",
    "$W$ – Controls for each sample (Confounders). Removes bias. These are variables that affect both the treatment ($T$) and the outcome ($Y$). We include them to ensure we are comparing \"apples to apples.\" However, we do not care if the treatment effect changes based on these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confounders ($W$) in Double ML, are used in the \"first stage\" to clean the data (residualization).\n",
    "\n",
    "Once the data is cleaned, the models only looks at $X$ to see how the effect changes.\n",
    "\n",
    "The model does not calculate \"causal importance\" for confounders because, by definition, we are \"controlling\" for them, not measuring their treatment effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your variables\n",
    "Y = samples_zones[target].values.ravel()\n",
    "T = samples_zones[treatment].values.ravel()\n",
    "W = samples_zones[vars_list]\n",
    "X = samples_zones[zone_vars]\n",
    "\n",
    "Y=Y.squeeze()\n",
    "T=T.squeeze()\n",
    "\n",
    "\n",
    "print(f\"Datasets shapes: Y={Y.shape}, T={T.shape}, W={W.shape}, X={X.shape},\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train estimator using LinearDML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# We use LinearDML because it is robust for estimating ATE\n",
    "est_linear = LinearDML(\n",
    "    model_y=AdaBoostClassifier(),          # add parameters if needed, e.g., n_estimators=100, learning_rate=0.1\n",
    "    model_t=LinearDiscriminantAnalysis(),  # add parameters if needed\n",
    "    discrete_treatment=True,\n",
    "    discrete_outcome=True,\n",
    "    cv=5,\n",
    "    random_state=123\n",
    ")\n",
    "# Note: In EconML, 'W' is for controls, 'X' is for heterogeneity. \n",
    "# If we don't want CATE, leave X out or pass None.\n",
    "est_linear.fit(Y, T, W=W, X=None)\n",
    "\n",
    "# Get the Average Treatment Effect\n",
    "print(f\"EconML Linear ATE: {est_linear.ate()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of causal inference, an Average Treatment Effect (ATE) represents the average difference in the outcome ($Y$) caused by the treatment ($T$), across the sampled population.\n",
    "\n",
    "The ATE tells you what would happen if you took your entire group and forced everyone to take the treatment, versus what would happen if you forced everyone to take the control.\n",
    "\n",
    "Mathematically: $ATE = E[Y(1) - Y(0)]\n",
    "\n",
    "On average, the treatment increases the outcome by $ATE$ units compared to not having the treatment.\n",
    "\n",
    "Since $Y$ is binary the ATE shows the percentage point increase in the probability of a drought impact.\n",
    "\n",
    "This ATE is specifically the causal effect after \"cleaning out\" the influence of your confounders ($W$)\n",
    "\n",
    "By using the Frisch-Waugh-Lovell approach, we have isolated the isolated impact of the treatment itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_linear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add $X$, we include regions to look for conditional ATE (CATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# We use LinearDML because it is robust for estimating ATE\n",
    "est_linear = LinearDML(\n",
    "    model_y=AdaBoostClassifier(),          # add parameters if needed, e.g., n_estimators=100, learning_rate=0.1\n",
    "    model_t=LinearDiscriminantAnalysis(),  # add parameters if needed\n",
    "    discrete_treatment=True,\n",
    "    discrete_outcome=True,\n",
    "    cv=5,\n",
    "    random_state=123\n",
    ")\n",
    "# Note: In EconML, 'W' is for controls, 'X' is for heterogeneity. \n",
    "est_linear.fit(Y, T, W=W, X=X)\n",
    "\n",
    "# Get the unique regions to see the effect for each\n",
    "unique_X = X.drop_duplicates()\n",
    "\n",
    "# Calculate the CATE for each unique region\n",
    "region_effects = est_linear.effect(unique_X)\n",
    "\n",
    "# Get Confidence Intervals for these specific regions\n",
    "lower, upper = est_linear.effect_interval(unique_X, alpha=0.05)\n",
    "\n",
    "# Create a clean summary\n",
    "cate_summary = unique_X.copy()\n",
    "cate_summary['CATE'] = region_effects\n",
    "cate_summary['CI_Lower'] = lower\n",
    "cate_summary['CI_Upper'] = upper\n",
    "\n",
    "display(cate_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_linear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a linear model (*LinearDML*), the *summary()* is showing the coefficients ($\\beta$) of the linear equation used to calculate those effects.\n",
    "\n",
    "We've told the model that treatment effect ($T$) varies based on $X$ (regions). Because we are using a linear parametric model, EconML is fitting this specific equation behind the scenes:\n",
    "$$\\text{Effect}(X) = \\beta_{1}(\\text{zone\\_1}) + \\beta_{2}(\\text{zone\\_2}) + \\dots + \\text{cate\\_intercept}$$\n",
    "\n",
    "- \\text{cate\\_intercept}: This is the baseline effect. If a region isn't explicitly called out or if all $X$ variables were zero, this would be the effect.\n",
    "- \\text{point\\_estimate}: These are the coefficients, relative shifts. A coefficient of $0.105$ for one zone means that the effect in that zone is $0.105$ higher than the baseline.\n",
    "\n",
    "When we run est_linear.effect(unique_X), EconML does the addition for us. For a pixel in one Zone, the calculation looks like this:\n",
    "\n",
    "$$\\text{CATE}_{\\text{Zone 1}} = \\text{cate\\_intercept} + \\text{coeff}_{\\text{zone\\_5}}$$\n",
    "$$\\text{CATE}_{\\text{Zone 1}} = \\text{cate\\_intercept} + \\text{point\\_estimate} = \\text{CATE} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train estimator using CausalForestDML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = CausalForestDML(\n",
    "    model_y=AdaBoostClassifier(),          # add parameters if needed, e.g., n_estimators=100, learning_rate=0.1\n",
    "    model_t=LinearDiscriminantAnalysis(),  # add parameters if needed\n",
    "    discrete_treatment=True,\n",
    "    discrete_outcome=True,\n",
    "    cv=5,\n",
    "    random_state=42)\n",
    "\n",
    "# Running without X (Average effect only)\n",
    "cf.fit(Y, T, X=X, W=W)\n",
    "\n",
    "print(f\"EconmML Causal Forest ATE: {cf.ate(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In causal inference, we cannot use standard $R^2$ or Accuracy because you never observe the \"ground truth\" (the counterfactual).\n",
    "The RScorer in EconML is a way to get around this using what is called the R-loss.\n",
    "\n",
    "Instead of comparing your model to the true effect (which is invisible), the RScorer measures how well your model explains the \"leftover\" variation in the data (the error).\n",
    "\n",
    "This corresponds to the extra variance of the outcome explained by introducing heterogeneity in the effect as captured by the cate model, as opposed to always predicting a constant effect. A negative score, means that the cate model performs even worse than a constant effect model and hints at overfitting during training of the cate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cf.score(Y, T, X=X, W=W)\n",
    "print(f\"\\nR-Scorer: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the CATE by Climate Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get unique climate regions from your test or train set\n",
    "unique_regions = X.drop_duplicates().sort_values(by=X.columns[0])\n",
    "\n",
    "# 2. Calculate the effect (CATE) and the confidence intervals\n",
    "# 'X' must be the same format as used in .fit()\n",
    "effects = cf.effect(unique_regions)\n",
    "conf_int = cf.effect_interval(unique_regions, T0=0, T1=1, alpha=0.05)\n",
    "\n",
    "# 3. Create a summary table\n",
    "summary = unique_regions.copy()\n",
    "summary['Estimated_Effect'] = effects\n",
    "summary['Lower_Bound'] = conf_int[0]\n",
    "summary['Upper_Bound'] = conf_int[1]\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['zone_name'] = summary[zone_vars].idxmax(axis=1)\n",
    "\n",
    "plot_causal_effects(summary, title=f'Causal Impact of {treatment[0]} by zone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Validation - Sensitivity analysis\n",
    "\n",
    "Since we don’t have a “causal test set” with a ground-truth, we must use refutation tests.\n",
    "\n",
    "- **Placebo treatment**: Replace the treatment variable with a random noise variable. The model must find a causal effect of zero. If it finds a significant effect, the model is flawed.\n",
    "\n",
    "- **Omitted variable test**: “How strong would an unmeasured confounder (one I forgot to include) have to be to make my causal effect go to zero?” If the answer is unrealistically strong, your finding is robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Validate the Nuisance Models** (The \"Residualizers\")\n",
    "\n",
    "We must ensure model_Y and model_T actually learned something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features\n",
    "# In EconML, when both X and W are provided, \n",
    "# the nuisance models are trained on the concatenation of [X, W]\n",
    "features = np.hstack([X.values, W.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the model with your desired parameters\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'random_state': 42\n",
    "}\n",
    "model_Y = AdaBoostClassifier(**params)\n",
    "\n",
    "# 2. Fit the model\n",
    "# We are predicting the outcome Y based on everything EXCEPT the treatment T\n",
    "model_Y.fit(features, Y)\n",
    "\n",
    "# 3. Generate \"Residuals\" (The part of Y the model couldn't explain)\n",
    "# For discrete outcomes, EconML typically looks at the difference between \n",
    "# the observed Y and the predicted probability\n",
    "y_pred_proba = model_Y.predict_proba(features)\n",
    "\n",
    "# Standard Classification Accuracy\n",
    "y_pred_class = model_Y.predict(features)\n",
    "accuracy = accuracy_score(Y, y_pred_class)\n",
    "print(f\"Model_Y Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# ROC AUC (Best for seeing how well the model separates classes)\n",
    "# Note: Use the probabilities for the positive class (usually index 1)\n",
    "y_prob_positive = y_pred_proba[:, 1] \n",
    "auc = roc_auc_score(Y, y_prob_positive)\n",
    "print(f\"Model_Y AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the model with your desired parameters\n",
    "model_T = LinearDiscriminantAnalysis()\n",
    "\n",
    "# 3. Fit the model\n",
    "# We are predicting the outcome Y based on everything EXCEPT the treatment T\n",
    "model_T.fit(features, Y)\n",
    "\n",
    "# 4. Generate \"Residuals\" (The part of Y the model couldn't explain)\n",
    "# For discrete outcomes, EconML typically looks at the difference between \n",
    "# the observed Y and the predicted probability\n",
    "t_pred_proba = model_T.predict_proba(features)\n",
    "\n",
    "# Standard Classification Accuracy\n",
    "t_pred_class = model_T.predict(features)\n",
    "accuracy = accuracy_score(T, t_pred_class)\n",
    "print(f\"Model_T Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# ROC AUC (Best for seeing how well the model separates classes)\n",
    "# Note: Use the probabilities for the positive class (usually index 1)\n",
    "t_prob_positive = t_pred_proba[:, 1] \n",
    "auc = roc_auc_score(Y, t_prob_positive)\n",
    "print(f\"Model_T AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The Placebo Validation (or \"Permutation Test\")**\n",
    "\n",
    "Checking if the model is just finding patterns in noise. If we shuffle the treatment assignments, the relationship between $T$ and $Y$ is broken. A robust Causal Forest should then report an ATE of zero. \n",
    "\n",
    "If the placebo ATE is similar to the real ATE, the model is likely picking up a \"spurious\" correlation.\n",
    "\n",
    "We shuffle the treatment vector $T$, re-fit the model (or just the effect part), and compare the \"Fake ATE\" to your \"Real ATE.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Create a shuffled version of the Treatment\n",
    "T_placebo = np.random.permutation(T)\n",
    "\n",
    "# 2. Initialize a \"Placebo Forest\" with identical settings\n",
    "cf_placebo = CausalForestDML(\n",
    "    model_y=AdaBoostClassifier(),\n",
    "    model_t=LinearDiscriminantAnalysis(),\n",
    "    discrete_treatment=True,\n",
    "    discrete_outcome=True,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Fit using the SHUFFLED treatment\n",
    "# We keep Y, X, and W exactly the same\n",
    "cf_placebo.fit(Y, T_placebo, X=X, W=W)\n",
    "\n",
    "real_ate = cf.ate(X)\n",
    "placebo_ate = cf_placebo.ate(X)\n",
    "\n",
    "# 4. Compare Results\n",
    "real_ate_val = real_ate.item()\n",
    "placebo_ate_val = placebo_ate.item()\n",
    "\n",
    "print(f\"Real ATE:    {real_ate_val:.6f}\")\n",
    "print(f\"Placebo ATE: {placebo_ate_val:.6f}\")\n",
    "\n",
    "# 5. Calculation of the 'Causal Signal-to-Noise' Ratio\n",
    "signal_to_noise = abs(real_ate_val / placebo_ate_val) if placebo_ate_val != 0 else np.inf\n",
    "print(f\"Signal-to-Noise Ratio: {signal_to_noise:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Subset Refuter or Leave-One-Confounder-Out check**\n",
    "\n",
    "In a stable causal model, removing a single confounder should not wildly swing the ATE unless that specific variable was the only thing holding the model's logic together.\n",
    "\n",
    "If the ATE changes drastically when you drop one variable, it suggests that the model is \"leaning\" too heavily on that feature to explain the relationship, which often points back to the overfitting issues we saw in the R-score.\n",
    "\n",
    "Implementation: The \"Leave-One-Out\" Sensitivity Check. We will loop through the list of control variables ($W$), remove one at a time, re-fit the forest, and record the new ATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify your confounders\n",
    "confounders = vars_list  # This is your W columns list\n",
    "results = []\n",
    "\n",
    "# 2. Iterate through each confounder and re-estimate ATE\n",
    "for col in confounders:\n",
    "    # Create a reduced W by dropping one column\n",
    "    W_reduced = W.drop(columns=[col])\n",
    "    \n",
    "    # Initialize the model again\n",
    "    cf_sens = CausalForestDML(\n",
    "        model_y=AdaBoostClassifier(), \n",
    "        model_t=LinearDiscriminantAnalysis(),\n",
    "        discrete_treatment=True,\n",
    "        discrete_outcome=True,\n",
    "        cv=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit and get new ATE\n",
    "    cf_sens.fit(Y, T, X=X, W=W_reduced)\n",
    "    new_ate = cf_sens.ate(X).item()\n",
    "    \n",
    "    results.append({'Dropped Confounder': col, 'New ATE': new_ate})\n",
    "\n",
    "# 3. Create a summary DataFrame\n",
    "df_sens = pd.DataFrame(results)\n",
    "df_sens.loc[len(df_sens)] = {'Dropped Confounder': 'None (Original)', 'New ATE': real_ate_val}\n",
    "\n",
    "# 4. Visualize the sensitivity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.axvline(x=real_ate_val, color='red', linestyle='--', label='Original ATE')\n",
    "plt.barh(df_sens['Dropped Confounder'], df_sens['New ATE'], color='skyblue')\n",
    "plt.xlabel('Estimated ATE')\n",
    "plt.title('Sensitivity Analysis: Dropping One Confounder')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_op",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
