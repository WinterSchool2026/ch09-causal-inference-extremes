{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Machine Learning (DML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DML is an algorithm that applies machine learning methods to fit the treatment and response, then uses a linear model to predict the response residuals from the treatment residuals.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WinterSchool2026/ch09-causal-inference-extremes/blob/main/notebooks/04_causal_models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade pip first for better dependency resolution\n",
    "!pip install -U pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages, ensuring numpy is at a version compatible with most 2024-2025 builds\n",
    "!pip install -q pycaret econml numba xarray zarr fsspec aiohttp geopandas dask netcdf4 h5netcdf \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "from econml.dml import LinearDML, CausalForestDML\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mount drive if you haven't already\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Append the PARENT directory (notebooks), not the utils folder itself\n",
    "path_to_parent = '/content/drive/MyDrive/09_challenge_EllisWinterSchool'\n",
    "if path_to_parent not in sys.path:\n",
    "    sys.path.append(path_to_parent)\n",
    "\n",
    "# 3. Now Python sees 'utils' as a package inside 'notebooks'\n",
    "import utils.utils\n",
    "from utils.utils import *\n",
    "\n",
    "print(\"✅ Success! Functions imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample data (trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv(\"/content/drive/MyDrive/09_challenge_EllisWinterSchool/df_ps_trimmed.csv\")\n",
    "\n",
    "print(f\"Shape of sampled data: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the variables: Outcome (target), Treatment, Heterogeneity (zones), Confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['DI_agri_extreme_M7']\n",
    "\n",
    "treatment = ['SMA_2']\n",
    "\n",
    "zones = ['basin_lv2']\n",
    "\n",
    "vars_list = ['E_gleam_ds','S_gleam_ds','H_gleam_ds',\n",
    "            'pev_ds','sro_ds','sp_ds','tp_ds','d2m_ds',\n",
    "            'agri_irri', 'agri_mix', 'agri_rain',\n",
    "            'soil_clay', 'soil_oc', 'soil_roots','soil_sand', 'soil_tawc',\n",
    "            'lst_night_ds','ndvi_ds','ndwi_ds',\n",
    "            'pop','road','hand','lc2','lc3','lc5','lc8',\n",
    "            'censo','soi_long','pdo_timeseries_sstens','noaa_globaltmp_comb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Xi (the heterogeneity features) - Create a one-hot ecoder (dummy variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_encoded = encode_categorical_raster(samples[zones[0]], prefix='zone')\n",
    "samples_zones = samples.join(zones_encoded)\n",
    "zone_vars = [v for v in samples_zones.columns if v.startswith(('zone'))]\n",
    "samples_zones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show proportion of true/negatives for treatment/outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix using crosstab\n",
    "# Rows = Treatment, Columns = Target\n",
    "ct = pd.crosstab(samples_zones[treatment[0]], samples_zones[target[0]])\n",
    "    \n",
    "# Extract values using .loc[row, col]\n",
    "NN = ct.loc[0, 0] # Treatment 0, Outcome 0\n",
    "NP = ct.loc[1, 0] # Treatment 1, Outcome 0 (T=1, O=0)\n",
    "PN = ct.loc[0, 1] # Treatment 0, Outcome 1 (T=0, O=1)\n",
    "PP = ct.loc[1, 1] # Treatment 1, Outcome 1 (T=1, O=1)\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append({\n",
    "    'Treatment': treatment,\n",
    "    '(T=0, O=0)': NN,\n",
    "    '(T=0, O=1)': PN,\n",
    "    '(T=1, O=0)': NP,\n",
    "    '(T=1, O=1)': PP,\n",
    "    'Tot_P_Treat': NP + PP,\n",
    "    'Tot_P_Out': PN + PP,\n",
    "})\n",
    "\n",
    "# 3. Display the summary dataframe\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(\"Outcome:\")\n",
    "print(samples_zones[target[0]].value_counts())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Treatment and Outcome for each Heterogenous zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize a list to store results for all zones\n",
    "target_col = target[0] if isinstance(target, list) else target\n",
    "all_zone_results = []\n",
    "\n",
    "# 2. Loop through each zone and calculate the confusion matrix\n",
    "for zone in zone_vars:\n",
    "    # Generate crosstab: Row = Zone Presence (0 or 1), Column = Outcome (0 or 1)\n",
    "    ct = pd.crosstab(samples_zones[zone], samples_zones[target_col])\n",
    "    \n",
    "    # Ensure all quadrants exist (2x2)\n",
    "    ct = ct.reindex(index=[0, 1], columns=[0, 1], fill_value=0)\n",
    "    \n",
    "    # Append the results for this specific zone\n",
    "    all_zone_results.append({\n",
    "        'Zone': zone,\n",
    "        '(T=0, O=0)': ct.loc[0, 0],  # Outside this zone, no outcome\n",
    "        '(T=0, O=1)': ct.loc[0, 1],  # Outside this zone, outcome exists\n",
    "        '(T=1, O=0)': ct.loc[1, 0],  # Inside this zone, no outcome\n",
    "        '(T=1, O=1)': ct.loc[1, 1]   # Inside this zone, outcome exists\n",
    "    })\n",
    "\n",
    "# 3. Create the consolidated summary DataFrame\n",
    "summary_df = pd.DataFrame(all_zone_results).set_index('Zone')\n",
    "\n",
    "# 4. Define colors\n",
    "# Blue/Light Blue for Negatives, Orange/Gold for Positives\n",
    "colors = ['#aec7e8', '#1f77b4', '#ebb95e', '#d18b00']\n",
    "\n",
    "# 5. Plot everything in one figure\n",
    "fig, ax = plt.subplots(figsize=(9, 5), dpi=100)\n",
    "\n",
    "summary_df[['(T=0, O=0)', '(T=0, O=1)', '(T=1, O=0)', '(T=1, O=1)']].plot(\n",
    "    kind='bar', \n",
    "    stacked=True, \n",
    "    color=colors, \n",
    "    ax=ax, \n",
    "    edgecolor='white',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# Formatting\n",
    "plt.title('Data Imbalance across Regions (Zones)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Observations', fontsize=12)\n",
    "plt.xlabel('Zone (heterogeneity)', fontsize=12)\n",
    "plt.legend(title='(Treatment,Outcome)', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize spatial bias (T/O) to check if we have enough representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spatial_bias(samples_zones, treatment[0], target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Machine Learning (EconML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Machine Learning (DML) is an algorithm that applies machine learning methods to fit the treatment and response, then uses a model to predict the response residuals from the treatment residuals.\n",
    "\n",
    "The EconML SDK implements the following DML classes:\n",
    "\n",
    "- **LinearDML**: suitable for estimating heterogeneous treatment effects.\n",
    "\n",
    "- **CausalForestDML**: ML algorithm used to estimate heterogeneous treatment effects (Conditional Average Treatment Effect, CATE). By combining Double Machine Learning (DML) with Causal Forests. It is a popular estimator for causal inference, designed to identify how a treatment effect varies across different segments of a population based on individual characteristics, even in high-dimensional settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y$ – Outcomes for each sample\n",
    "\n",
    "$T$ – Treatments for each sample\n",
    "\n",
    "$X$ – Features for each sample (Regions, heterogeneity). To find variation (CATE). These are the variables where you suspect the treatment effect might be different. Encoded one-hot\n",
    "\n",
    "$W$ – Controls for each sample (Confounders). Removes bias. These are variables that affect both the treatment ($T$) and the outcome ($Y$). We include them to ensure we are comparing \"apples to apples.\" However, we do not care if the treatment effect changes based on these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confounders ($W$) in Double ML, are used in the \"first stage\" to clean the data (residualization).\n",
    "\n",
    "Once the data is cleaned, the models only looks at $X$ to see how the effect changes.\n",
    "\n",
    "The model does not calculate \"causal importance\" for confounders because, by definition, we are \"controlling\" for them, not measuring their treatment effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your variables\n",
    "Y = samples_zones[target].values.ravel()\n",
    "T = samples_zones[treatment].values.ravel()\n",
    "W = samples_zones[vars_list]\n",
    "X = samples_zones[zone_vars]\n",
    "\n",
    "Y=Y.squeeze()\n",
    "T=T.squeeze()\n",
    "\n",
    "\n",
    "print(f\"Datasets shapes: Y={Y.shape}, T={T.shape}, W={W.shape}, X={X.shape},\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train estimator using LinearDML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: train your model here without the heterogeneity variables (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of causal inference, an Average Treatment Effect (ATE) represents the average difference in the outcome ($Y$) caused by the treatment ($T$), across the sampled population.\n",
    "\n",
    "The ATE tells you what would happen if you took your entire group and forced everyone to take the treatment, versus what would happen if you forced everyone to take the control.\n",
    "\n",
    "Mathematically: $ATE = E[Y(1) - Y(0)]\n",
    "\n",
    "On average, the treatment increases the outcome by $ATE$ units compared to not having the treatment.\n",
    "\n",
    "Since $Y$ is binary the ATE shows the percentage point increase in the probability of a drought impact.\n",
    "\n",
    "This ATE is specifically the causal effect after \"cleaning out\" the influence of your confounders ($W$)\n",
    "\n",
    "By using the Frisch-Waugh-Lovell approach, we have isolated the isolated impact of the treatment itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_linear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add $X$, we include regions to look for conditional ATE (CATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: train your Linear model here with the heterogeneity variables (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_linear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a linear model (*LinearDML*), the *summary()* is showing the coefficients ($\\beta$) of the linear equation used to calculate those effects.\n",
    "\n",
    "We've told the model that treatment effect ($T$) varies based on $X$ (regions). Because we are using a linear parametric model, EconML is fitting this specific equation behind the scenes:\n",
    "$$\\text{Effect}(X) = \\beta_{1}(\\text{zone\\_1}) + \\beta_{2}(\\text{zone\\_2}) + \\dots + \\text{cate\\_intercept}$$\n",
    "\n",
    "- \\text{cate\\_intercept}: This is the baseline effect. If a region isn't explicitly called out or if all $X$ variables were zero, this would be the effect.\n",
    "- \\text{point\\_estimate}: These are the coefficients, relative shifts. A coefficient of $0.105$ for one zone means that the effect in that zone is $0.105$ higher than the baseline.\n",
    "\n",
    "When we run est_linear.effect(unique_X), EconML does the addition for us. For a pixel in one Zone, the calculation looks like this:\n",
    "\n",
    "$$\\text{CATE}_{\\text{Zone 1}} = \\text{cate\\_intercept} + \\text{coeff}_{\\text{zone\\_5}}$$\n",
    "$$\\text{CATE}_{\\text{Zone 1}} = \\text{cate\\_intercept} + \\text{point\\_estimate} = \\text{CATE} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train estimator using CausalForestDML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: train your CausalForest model here with the heterogeneity variables (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In causal inference, we cannot use standard $R^2$ or Accuracy because you never observe the \"ground truth\" (the counterfactual).\n",
    "The RScorer in EconML is a way to get around this using what is called the R-loss.\n",
    "\n",
    "Instead of comparing your model to the true effect (which is invisible), the RScorer measures how well your model explains the \"leftover\" variation in the data (the error).\n",
    "\n",
    "This corresponds to the extra variance of the outcome explained by introducing heterogeneity in the effect as captured by the cate model, as opposed to always predicting a constant effect. A negative score, means that the cate model performs even worse than a constant effect model and hints at overfitting during training of the cate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cf.score(Y, T, X=X, W=W)\n",
    "print(f\"\\nR-Scorer: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the CATE by Climate Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: visualize CATE with confidence intervals, prepare the data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['zone_name'] = summary[zone_vars].idxmax(axis=1)\n",
    "\n",
    "plot_causal_effects(summary, title=f'Causal Impact of {treatment[0]} by zone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Validation - Sensitivity analysis\n",
    "\n",
    "Since we don’t have a “causal test set” with a ground-truth, we must use refutation tests.\n",
    "\n",
    "- **Placebo treatment**: Replace the treatment variable with a random noise variable. The model must find a causal effect of zero. If it finds a significant effect, the model is flawed.\n",
    "\n",
    "- **Omitted variable test**: “How strong would an unmeasured confounder (one I forgot to include) have to be to make my causal effect go to zero?” If the answer is unrealistically strong, your finding is robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Validate the Nuisance Models** (The \"Residualizers\")\n",
    "\n",
    "We must ensure model_Y and model_T actually learned something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features\n",
    "# In EconML, when both X and W are provided, the nuisance models are trained on the concatenation of [X, W]\n",
    "features = np.hstack([X.values, W.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: measure accuracy and ROC AUC for residual models (nuisance functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The Placebo Validation (or \"Permutation Test\")**\n",
    "\n",
    "Checking if the model is just finding patterns in noise. If we shuffle the treatment assignments, the relationship between $T$ and $Y$ is broken. A robust Causal Forest should then report an ATE of zero. \n",
    "\n",
    "If the placebo ATE is similar to the real ATE, the model is likely picking up a \"spurious\" correlation.\n",
    "\n",
    "We shuffle the treatment vector $T$, re-fit the model (or just the effect part), and compare the \"Fake ATE\" to your \"Real ATE.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: compare ATE estimates between the real treatment and a placebo treatment\n",
    "\n",
    "print(f\"Real ATE:    {real_ate_val:.6f}\")\n",
    "print(f\"Placebo ATE: {placebo_ate_val:.6f}\")\n",
    "\n",
    "# 5. Calculation of the 'Causal Signal-to-Noise' Ratio\n",
    "signal_to_noise = abs(real_ate_val / placebo_ate_val) if placebo_ate_val != 0 else np.inf\n",
    "print(f\"Signal-to-Noise Ratio: {signal_to_noise:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Subset Refuter or Leave-One-Confounder-Out check**\n",
    "\n",
    "In a stable causal model, removing a single confounder should not wildly swing the ATE unless that specific variable was the only thing holding the model's logic together.\n",
    "\n",
    "If the ATE changes drastically when you drop one variable, it suggests that the model is \"leaning\" too heavily on that feature to explain the relationship, which often points back to the overfitting issues we saw in the R-score.\n",
    "\n",
    "Implementation: The \"Leave-One-Out\" Sensitivity Check. We will loop through the list of control variables ($W$), remove one at a time, re-fit the forest, and record the new ATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify your confounders\n",
    "confounders = vars_list  # This is your W columns list\n",
    "results = []\n",
    "\n",
    "# 2. Iterate through each confounder and re-estimate ATE\n",
    "for col in confounders:\n",
    "\n",
    "    ## TODO: drop a condounder, re-train your model, and re-estimate ATE\n",
    "\n",
    "# 3. Create a summary DataFrame\n",
    "df_sens = pd.DataFrame(results)\n",
    "df_sens.loc[len(df_sens)] = {'Dropped Confounder': 'None (Original)', 'New ATE': real_ate_val}\n",
    "\n",
    "# 4. Visualize the sensitivity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.axvline(x=real_ate_val, color='red', linestyle='--', label='Original ATE')\n",
    "plt.barh(df_sens['Dropped Confounder'], df_sens['New ATE'], color='skyblue')\n",
    "plt.xlabel('Estimated ATE')\n",
    "plt.title('Sensitivity Analysis: Dropping One Confounder')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_op",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
