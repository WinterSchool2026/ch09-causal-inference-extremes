{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity Scores (PS)\n",
    "\n",
    "PS offer a solution to mimic a randomized controlled trial (RCT) using observational data.\n",
    "\n",
    "In an RCT, we’d randomly assign some pixels to be treated and others to be control. Because of randomization, the two groups would be identical on average in all other characteristics (same soil, same elevation, same prior climate).\n",
    "In ESS, we can’t do this. For example, the pixels that were reforested were chosen for a reason (e.g., policy, good soil, near a road). This creates selection bias.\n",
    "\n",
    "Matching methods are designed to correct this selection bias by selecting samples to find similar comparisons. However, we will use PS to manually discard extreme samples that have a very high or very low probability of being treated using logistic regression. We wont use matching methods.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WinterSchool2026/ch09-causal-inference-extremes/blob/main/notebooks/02_data_filtering_propensity_score.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade pip first for better dependency resolution\n",
    "!pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages, ensuring numpy is at a version compatible with most 2024-2025 builds\n",
    "!pip install -q econml numba xarray zarr fsspec aiohttp geopandas dask netcdf4 h5netcdf \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters data to find the best examples for comparison. A propensity score is a number for each sample that answers the question: Given all the confounders for this sample, what was the probability that it would receive the treatment? It's a propensity or tendency to be treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount the folder with the utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mount drive if you haven't already\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Append the PARENT directory (notebooks), not the utils folder itself\n",
    "path_to_parent = '/content/drive/MyDrive/09_challenge_EllisWinterSchool'\n",
    "if path_to_parent not in sys.path:\n",
    "    sys.path.append(path_to_parent)\n",
    "\n",
    "# 3. Now Python sees 'utils' as a package inside 'notebooks'\n",
    "import utils.utils\n",
    "from utils.utils import *\n",
    "\n",
    "print(\"✅ Success! Functions imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the sample points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv('/content/drive/MyDrive/09_challenge_EllisWinterSchool/dataset_clim_env_oci_norm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_country = gpd.read_file('/content/drive/MyDrive/09_challenge_EllisWinterSchool/world-administrative-boundaries.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logisitic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PS we will train a classifier that uses the control variables to predict the Treatment (T).\n",
    "\n",
    "- Step 1: train a classifier, logistic regression, to predict the treatment (T) using confounders (W), the output is the PS for each sample.\n",
    "- Step 2: Filter. Filter out samples that have propensity score above *0.90/0.95* which obvious it would be treated, and below *0.1/0.05* very unlikely to be treated.\n",
    "\n",
    "This step has to be done using the selected factors (confounders). If we change the confounders the PS has to be done again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vars = ['E_gleam_ds','S_gleam_ds','H_gleam_ds',\n",
    "            'pev_ds','sro_ds','sp_ds','tp_ds','d2m_ds',\n",
    "            'agri_irri', 'agri_mix', 'agri_rain',\n",
    "            'soil_clay', 'soil_oc', 'soil_roots','soil_sand', 'soil_tawc',\n",
    "            'lst_night_ds','ndvi_ds','ndwi_ds',\n",
    "            'pop','road','hand','lc2','lc3','lc5','lc8',\n",
    "            'censo','soi_long','pdo_timeseries_sstens','noaa_globaltmp_comb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = \"SMA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the TREATMENTS\n",
    "samples['SMA_2'] = np.where(samples['SMA'] >= 2, 1, 0)\n",
    "samples = samples.drop(treatment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = \"SMA_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_spatial_temporal_grid(samples, treatment, bound_country)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use Soil Moisture Anomaly (SMA) as a treatment (T), where SMA >= 2 is treated and < 2 non-treated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_vars = [\"DI_agri_extreme_M7\",\"id\", treatment]  + inp_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ps = samples[ps_vars].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the means for control and treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ps.groupby(treatment).mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate control and treatment for t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control = df_ps[df_ps[treatment] == 0]\n",
    "df_treatment = df_ps[df_ps[treatment] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student's t-test for revenue (dependent variable)\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "print(df_control.DI_agri_extreme_M7.mean(), df_treatment.DI_agri_extreme_M7.mean())\n",
    "\n",
    "# compare samples\n",
    "_, p = ttest_ind(df_control.DI_agri_extreme_M7, df_treatment.DI_agri_extreme_M7)\n",
    "print(f'p={p:.3f}')\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05  # significance level\n",
    "if p > alpha:\n",
    "    print('same distributions/same group mean (fail to reject H0 - we do not have enough evidence to reject H0)')\n",
    "else:\n",
    "    print('different distributions/different group mean (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate logistic propensity scores.\n",
    "\n",
    "We fit a logistic regression to our pixels with all our variables, and we are trying to predict our treatment (T). Unlike most machine learning tasks we don’t calculate the accuracy here. It is the predicted proability we are interested in.\n",
    "\n",
    "Propensity Score is the raw predicted probability from a logistic regression (logit) model, indicating the likelihood of an unit being in the treatment group based on covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "## TODO: train your logistic regression model here\n",
    "\n",
    "print(f\"Treatment Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Treatment Model AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting density functions for treated and untreated groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted_data attribute contains the 'propensity_score' column\n",
    "sns.kdeplot(data=df_ps[df_ps[treatment] == 1], \n",
    "            x='prop_score', label='Treated', fill=True)\n",
    "sns.kdeplot(data=df_ps[df_ps[treatment] == 0], \n",
    "            x='prop_score', label='Untreated (Original)', fill=True)\n",
    "\n",
    "plt.title('Propensity Score Density')\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out samples with extreme propensity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify the IDs that fall within the common support\n",
    "valid_ids = df_ps.loc[\n",
    "    (df_ps[\"prop_score\"] > 0.05) & (df_ps[\"prop_score\"] < 0.95), \"id\"\n",
    "]\n",
    "print(np.unique(valid_ids).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Select rows from the original samples where the 'id' is in our valid list\n",
    "# This ensures you keep every single original column.\n",
    "df_trimmed = samples[samples[\"id\"].isin(valid_ids)]\n",
    "df_trimmed_plot = df_ps[df_ps[\"id\"].isin(valid_ids)]\n",
    "\n",
    "# Print the diagnostics\n",
    "removed_count = len(samples) - len(df_trimmed)\n",
    "print(f\"Removed {removed_count} samples due to extreme propensity scores.\")\n",
    "print(f\"New dataset size: {len(df_trimmed)} samples.\")\n",
    "\n",
    "# Save to a new CSV file\n",
    "df_trimmed.to_csv(\"/content/drive/MyDrive/09_challenge_EllisWinterSchool/df_ps_trimmed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted_data attribute contains the 'propensity_score' column\n",
    "sns.kdeplot(data=df_trimmed_plot[df_trimmed_plot[treatment] ==1], \n",
    "            x='prop_score', label='Treated', fill=True)\n",
    "sns.kdeplot(data=df_trimmed_plot[df_trimmed_plot[treatment] ==0], \n",
    "            x='prop_score', label='Untreated (Original)', fill=True)\n",
    "\n",
    "plt.title('Propensity Score Density')\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check stats and the spatial representation of the trimmed sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_split_violin_mosaic(df_trimmed, target_var=\"DI_agri_extreme_M7\", \n",
    "                          define_features_list=inp_vars, ncols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of the Outcome and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_spatial_temporal_grid(df_trimmed, treatment, bound_country)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_spatial_temporal_grid(df_trimmed, \"DI_agri_extreme_M7\", bound_country)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_op",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
